{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "def top_k_top_p_filter(logits: torch.Tensor, top_k: int = 0, top_p: float = 0.0):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensorpe_): 2D tensor with shape (batch, vocab)\n",
    "        top_k (int, optional): top_k. Defaults to 0.\n",
    "        top_p (float, optional): top_p. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: a renormalized logits\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        filter = torch.topk(logits, min(top_k, logits.size(-1)))[0]\n",
    "        logits[logits < filter[:, [-1]]] = float('-inf')\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(\n",
    "            F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        filter = cumulative_probs > top_p\n",
    "        filter[..., 1:] = filter[..., :-1].clone()\n",
    "        filter[..., 0] = 0\n",
    "        indices_to_remove = filter.scatter(1, sorted_indices, filter)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "\n",
    "def norm_logits(logits : torch.Tensor, temperature : float, top_k : float, top_p : float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): shape (1, vocab)\n",
    "        temperature (float): temperature\n",
    "        top_k (float): top_k\n",
    "        top_p (float): top_p\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: next token with shape as (batch,  1)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 2\n",
    "    logits = logits / temperature\n",
    "    logits = top_k_top_p_filter(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def sample(probs : torch.Tensor, num_samples: int = 1):\n",
    "    idx_next = torch.multinomial(probs, num_samples=num_samples)\n",
    "    if (idx_next.item() == 0):\n",
    "        raise RuntimeError\n",
    "    return idx_next\n",
    "\n",
    "\n",
    "def max_fn(x):\n",
    "    \"\"\"\n",
    "        norm(max (x, 0))\n",
    "    \"\"\"\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    x_max_sum = torch.sum(x_max, dim=1, keepdim=True) \n",
    "    return x_max / x_max_sum\n",
    "\n",
    "@torch.no_grad()\n",
    "def autoregressive_sampling(x : torch.Tensor, model : torch.nn.Module, N : int, \n",
    "                            temperature : float = 1, top_k : int = 0, top_p : float = 0):\n",
    "    n = len(x)\n",
    "    T = len(x) + N\n",
    "\n",
    "    past_key_values = None\n",
    "    while n < T:\n",
    "        # outputs = model(x)\n",
    "        if past_key_values:\n",
    "            last_ids = x[:, -1]\n",
    "            if last_ids.dim() == 1:\n",
    "                last_ids = torch.unsqueeze(last_ids, 0)\n",
    "            outputs = model(last_ids, past_key_values = past_key_values, use_cache = True)\n",
    "        else:\n",
    "            outputs = model(x)\n",
    "        last_p = norm_logits(outputs.logits[::, -1, :], temperature, top_k, top_p)\n",
    "        past_key_values = outputs.past_key_values\n",
    "        idx_next = sample(last_p)\n",
    "        x = torch.cat((x, idx_next), dim=1)\n",
    "        n += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _debug_show_kvcache(past_key_values):\n",
    "    if  past_key_values is None:\n",
    "        return\n",
    "    for elem in past_key_values:\n",
    "        k, v = elem\n",
    "        print(f\"kv cache: k shape {k.shape}, v shape {v.shape}\")\n",
    "        break\n",
    "\n",
    "class KVCacheModel(torch.nn.Module):\n",
    "    def __init__(self, model : torch.nn.Module, temperature : float = 1, top_k : int = 0, top_p : float = 0) -> None:\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "        self._past_key_values = None\n",
    "        self._prob_history = None\n",
    "\n",
    "        self._temperature = temperature\n",
    "        self._top_k = top_k\n",
    "        self._top_p = top_p\n",
    "\n",
    "    def _forward_with_kvcache(self, input_ids : torch.Tensor, use_debug = True) -> torch.Tensor:\n",
    "        if self._past_key_values is None:\n",
    "            assert self._prob_history is None, f\"{self._prob_history.shape}\"\n",
    "            # the first forward (prefill) returns the prompt's logits\n",
    "            outputs = self._model(input_ids)\n",
    "            self._prob_history = outputs.logits\n",
    "            for i in range(self._prob_history.shape[-2]):   \n",
    "                self._prob_history[:, i, :] = norm_logits(self._prob_history[:, i, :], self._temperature, self._top_k, self._top_p)\n",
    "            self._past_key_values = outputs.past_key_values\n",
    "            last_q = self._prob_history[:, -1, :]\n",
    "        else:\n",
    "            # return the last token's logits\n",
    "            cached_len = 0\n",
    "            for kv in self._past_key_values:\n",
    "                k, v = kv\n",
    "                cached_len = k.shape[2]\n",
    "                \n",
    "            last_input_id = input_ids[:, cached_len:]\n",
    "            if last_input_id.dim() == 1:\n",
    "                last_input_id = torch.unsqueeze(last_input_id, 0)\n",
    "            \n",
    "            if use_debug:\n",
    "                print(f\"last_input_id shape {last_input_id.shape}\")\n",
    "                _debug_show_kvcache(self._past_key_values)\n",
    "            \n",
    "            outputs = self._model(last_input_id, past_key_values=self._past_key_values, use_cache=True)\n",
    "            \n",
    "            not_cached_q = outputs.logits\n",
    "            if not_cached_q.dim() == 2:\n",
    "                not_cached_q = torch.unsqueeze(not_cached_q, 0)\n",
    "                \n",
    "            for i in range(not_cached_q.shape[-2]):   \n",
    "                not_cached_q[:, i, :] = norm_logits(not_cached_q[:, i, :], self._temperature, self._top_k, self._top_p)    \n",
    "                \n",
    "            self._prob_history = torch.cat([self._prob_history, not_cached_q], dim=1)\n",
    "            \n",
    "            last_q = not_cached_q[:, -1, :]\n",
    "            self._past_key_values = outputs.past_key_values\n",
    "        \n",
    "        return last_q\n",
    "\n",
    "\n",
    "    def _generate_with_kvcache(self, prefix : torch.Tensor, \n",
    "                                    gamma : int, \n",
    "                                    use_debug = False) -> torch.Tensor:\n",
    "        \"\"\" forward the model gamma times\n",
    "\n",
    "        Args:\n",
    "            prefix (torch.Tensor): the prefix\n",
    "            gamma (int): how many times approx guesses\n",
    "\n",
    "        Returns:\n",
    "            Torch.Tensor: prefix+generated tokens\n",
    "        \"\"\"\n",
    "        x = prefix\n",
    "\n",
    "        for _ in range(gamma):\n",
    "            q = self._forward_with_kvcache(x, use_debug)\n",
    "            next_tok = sample(q)\n",
    "            x = torch.cat((x, next_tok), dim=1)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input : torch.Tensor, gamma : int) -> torch.Tensor:\n",
    "        output = self._generate_with_kvcache(input, gamma)\n",
    "        return output\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def rollback(self, end_pos : int):\n",
    "        past_key_values_trimmed = []\n",
    "        assert self._past_key_values\n",
    "        for kv in self._past_key_values:\n",
    "            k, v = kv\n",
    "            \n",
    "            # k, v (batch, head, seq, hidden_dim)\n",
    "            k = k[:, :, :end_pos, :]\n",
    "            v = v[:, :, :end_pos, :]\n",
    "            kv_trimmed = (k, v)\n",
    "            past_key_values_trimmed.append(kv_trimmed)\n",
    "        \n",
    "        self._past_key_values = past_key_values_trimmed\n",
    "        self._prob_history = self._prob_history[:, :end_pos, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_sampling(prefix : torch.Tensor, approx_model : torch.nn.Module, target_model : torch.nn.Module, \n",
    "                         max_len : int , gamma : int = 4,\n",
    "                         temperature : float = 1, top_k : int = 0, top_p : float = 0, verbose : bool = False, random_seed : int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Google version Speculative Sampling.\n",
    "    https://arxiv.org/pdf/2211.17192.pdf\n",
    "        \n",
    "    Adapted with KV Cache Optimization.\n",
    "        \n",
    "    Args:\n",
    "        x (torch.Tensor): input sequence, (batch, prefix_seqlen), Note that the batch dim is always 1 now.\n",
    "        approx_model (torch.nn.Module): approx model, the small one\n",
    "        target_model (torch.nn.Module): target model, the large one\n",
    "        max_len (int): the max overall generated tokens number.\n",
    "        gamma (int): $\\gamma$, the token number small model guesses.\n",
    "        temperature (float, optional): Defaults to 1.\n",
    "        top_k (int, optional): Defaults to 0.\n",
    "        top_p (float, optional): Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: generated tokens (batch, target_seqlen)\n",
    "    \"\"\"\n",
    "    seq_len = prefix.shape[1]\n",
    "    T = seq_len + max_len\n",
    "    \n",
    "    assert prefix.shape[0] == 1, \"input batch size must be 1\"\n",
    "\n",
    "#     assert approx_model.device == target_model.device\n",
    "    \n",
    "    device = target_model.device\n",
    "    \n",
    "    approx_model_cache = KVCacheModel(approx_model, temperature, top_k, top_p).to('cuda:1')\n",
    "    target_model_cache = KVCacheModel(target_model, temperature, top_k, top_p).to('cuda:0')\n",
    "    \n",
    "    resample_count = 0\n",
    "    target_sample_count = 0\n",
    "    accepted_count = 0\n",
    "    start_time = time.time()\n",
    "    while prefix.shape[1] < T:\n",
    "        # q = M_q[prefix + x_0, x_1, .., x_(gamma-2)]\n",
    "        \n",
    "        prefix_len = prefix.shape[1]\n",
    "#         print(f'prefix_len was in which device {prefix_len.device}')\n",
    "        x = approx_model_cache.generate(prefix, gamma)\n",
    "        x = x.to('cuda:0')\n",
    "        _ = target_model_cache.generate(x, 1)\n",
    "        target_model_history = target_model_cache._prob_history\n",
    "        target_model_history = target_model_history.to('cuda:1')\n",
    "        n = prefix_len + gamma - 1\n",
    "        \n",
    "\n",
    "        for i in range(gamma):\n",
    "            if random_seed:\n",
    "                torch.manual_seed(random_seed)\n",
    "            r = torch.rand(1, device = 'cuda:1')\n",
    "            j = x[:, prefix_len + i]\n",
    "            j = j.to('cpu')\n",
    "            \n",
    "            if r > (target_model_history[:, prefix_len + i - 1, j]) / (approx_model_cache._prob_history[:, prefix_len + i - 1, j]):\n",
    "                # reject\n",
    "                n = prefix_len + i - 1\n",
    "                break\n",
    "            \n",
    "            # if verbose:\n",
    "            #     print(f\"approx guess accepted {j[0]}: \\033[31m{AutoTokenizer.decode(torch.tensor([j]))}\\033[0m\")\n",
    "\n",
    "            accepted_count += 1\n",
    "        \n",
    "        # print(f\"n : {n}, i : {i}, prefix_len + gamma - 1: {prefix_len + gamma - 1}\")\n",
    "        assert n >= prefix_len - 1, f\"n {n}, prefix_len {prefix_len}\"\n",
    "        prefix = x[:, :n + 1]\n",
    "        \n",
    "        approx_model_cache.rollback(n+1)\n",
    "        \n",
    "        assert approx_model_cache._prob_history.shape[-2] <= n + 1, f\"approx_model prob list shape {approx_model_cache._prob_history.shape}, n {n}\"\n",
    "        \n",
    "        if n < prefix_len + gamma - 1:\n",
    "            # reject someone, sample from the pos n\n",
    "            t = sample(max_fn(target_model_history[:, n, :] - approx_model_cache._prob_history[:, n, :]))\n",
    "            # if verbose:\n",
    "            #     print(f\"target resamples at position {n}: \\033[34m{tokenizer.decode(t)}\\033[0m\")\n",
    "            resample_count += 1\n",
    "            target_model_cache.rollback(n+1)\n",
    "        else:\n",
    "            # all approx model decoding accepted\n",
    "            assert n == target_model_history.shape[1] - 1\n",
    "            t = sample(target_model_history[:, -1, :])\n",
    "            # if verbose:\n",
    "            #     print(f\"target samples {n}: \\033[35m{tokenizer.decode(t)}\\033[0m\")\n",
    "            target_sample_count += 1\n",
    "            target_model_cache.rollback(n+2)\n",
    "        prefix = prefix.to(\"cuda:1\")\n",
    "#         print(f'prefix device is {prefix.device}, t device is {t.device}')\n",
    "        prefix = torch.cat((prefix, t), dim=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"generated tokens numbers {prefix.shape[-1] - seq_len}, accepted_count {accepted_count}, target_sample_count {target_sample_count}, resample_count {resample_count}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Token Generation Speed (with speculative decoding): {max_len/(end_time-start_time)} tokens/s\")\n",
    "    print(f\"Acceptance Rate: {accepted_count/max_len}\")\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/transformers/miniconda3/envs/transformers/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the following results use KV cache\n",
      "Token Generation Speed (with speculative decoding and hand-written KV cache): 40.805526376195054 tokens/s\n",
      "Acceptance Rate: 0.828\n",
      "\n",
      "Token Generation Speed (without speculative decoding and huggingface intergrated KV cache): 35.802769140912254 tokens/s\n"
     ]
    }
   ],
   "source": [
    "approx_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "approx_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", trust_remote_code=True)\n",
    "approx_model.eval()\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\",torch_dtype=\"auto\", trust_remote_code=True)\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\", trust_remote_code=True)\n",
    "target_model.eval()\n",
    "\n",
    "input_ids = target_tokenizer.encode(\"Please write an introduction about UC Irvine: \", return_tensors='pt')\n",
    "input_ids = input_ids.to('cuda:1')\n",
    "inputs = target_tokenizer(\"Please write an introduction about UC Irvine: \", return_tensors=\"pt\", return_attention_mask=False)\n",
    "top_k = 20\n",
    "top_p = 0.9\n",
    "\n",
    "output = speculative_sampling(input_ids, approx_model, target_model, max_len=500, gamma = 7, top_k = top_k, top_p=top_p, random_seed = 123, verbose = False)\n",
    "generated_text = target_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "start_time = time.time()\n",
    "target_model.generate(**inputs, max_length=500)\n",
    "end_time = time.time()\n",
    "print()\n",
    "print(f\"Token Generation Speed (without speculative decoding and huggingface intergrated KV cache): {500/(end_time-start_time)} tokens/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
