{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "# from torch.multiprocessing import Process\n",
    "from  time import sleep\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import time\n",
    "from utilis import KVCacheModel, sample, max_fn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def edge_speculative_sampling(prefix : torch.Tensor, \n",
    "                         approx_model : torch.nn.Module, \n",
    "                         SERVER_IP: str,\n",
    "                         max_len : int , gamma : int = 4,\n",
    "                         temperature : float = 1, top_k : int = 0, top_p : float = 0, \n",
    "                         verbose : bool = False, random_seed : int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    the edge speculative sample will handle\n",
    "    1. place prefix to device\n",
    "    2. place model to device\n",
    "    \"\"\"\n",
    "    seq_len = prefix.shape[1]\n",
    "    ## number of total token should generate\n",
    "    T = seq_len+max_len\n",
    "    approx_model_cache = KVCacheModel(approx_model, temperature, top_k, top_p).to('cuda:0')\n",
    "\n",
    "    #### stats collection ########\n",
    "    resample_count = 0\n",
    "    target_sample_count = 0\n",
    "    accepted_count = 0\n",
    "    prefix = prefix.to('cuda:0')\n",
    "    start_time = time.time()\n",
    "    #### stats collection ########\n",
    "\n",
    "    ## prefix = shape (1,prefix.len)\n",
    "    while prefix.shape[1] < T: \n",
    "        # update prefix len, it will update during each iteration\n",
    "        prefix_len = prefix.shape[1]\n",
    "        draft_tokens = approx_model_cache.generate(prefix, gamma)\n",
    "        draft_tokens_to_server = draft_tokens.to('cpu')\n",
    "        # draft_token_shape = list(draft_tokens.size())\n",
    "        draft_token_list = draft_tokens_to_server.tolist()\n",
    "        send_tensor_to_server(SERVER_IP=SERVER_IP,tensor_list=draft_token_list)\n",
    "        received_tensor = get_tensor(SERVER_IP=SERVER_IP)\n",
    "        #############\n",
    "        # how to wait until I get new tensor? \n",
    "        # I json error from get_tensor\n",
    "        #############\n",
    "        while received_tensor is None: \n",
    "            received_tensor = get_tensor(SERVER_IP=SERVER_IP)\n",
    "        \n",
    "        target_model_history = received_tensor\n",
    "        target_model_history = target_model_history.to('cuda:0')\n",
    "        n = prefix_len + gamma - 1\n",
    "        \n",
    "\n",
    "        for i in range(gamma):\n",
    "            if random_seed:\n",
    "                torch.manual_seed(random_seed)\n",
    "            r = torch.rand(1, device = 'cuda:0')\n",
    "            j = draft_tokens[:, prefix_len + i]\n",
    "            # j = j.to('cpu')\n",
    "            \n",
    "            if r > (target_model_history[:, prefix_len + i - 1, j]) / (approx_model_cache._prob_history[:, prefix_len + i - 1, j]):\n",
    "                # reject\n",
    "                n = prefix_len + i - 1\n",
    "                break\n",
    "            \n",
    "            # if verbose:\n",
    "            #     print(f\"approx guess accepted {j[0]}: \\033[31m{AutoTokenizer.decode(torch.tensor([j]))}\\033[0m\")\n",
    "\n",
    "            accepted_count += 1\n",
    "        \n",
    "        # print(f\"n : {n}, i : {i}, prefix_len + gamma - 1: {prefix_len + gamma - 1}\")\n",
    "        assert n >= prefix_len - 1, f\"n {n}, prefix_len {prefix_len}\"\n",
    "        prefix = draft_tokens[:, :n + 1]\n",
    "        \n",
    "        approx_model_cache.rollback(n+1)\n",
    "        \n",
    "        assert approx_model_cache._prob_history.shape[-2] <= n + 1, f\"approx_model prob list shape {approx_model_cache._prob_history.shape}, n {n}\"\n",
    "        \n",
    "        if n < prefix_len + gamma - 1:\n",
    "            # reject someone, sample from the pos n\n",
    "            t = sample(max_fn(target_model_history[:, n, :] - approx_model_cache._prob_history[:, n, :]))\n",
    "            # if verbose:\n",
    "            #     print(f\"target resamples at position {n}: \\033[34m{tokenizer.decode(t)}\\033[0m\")\n",
    "            resample_count += 1\n",
    "            update_target_cache(SERVER_IP,n+1)\n",
    "            # target_model_cache.rollback(n+1)\n",
    "        else:\n",
    "            # all approx model decoding accepted\n",
    "            assert n == target_model_history.shape[1] - 1\n",
    "            t = sample(target_model_history[:, -1, :])\n",
    "            # if verbose:\n",
    "            #     print(f\"target samples {n}: \\033[35m{tokenizer.decode(t)}\\033[0m\")\n",
    "            target_sample_count += 1\n",
    "            update_target_cache(SERVER_IP,n+2)\n",
    "            # target_model_cache.rollback(n+2)\n",
    "        prefix = prefix.to(\"cuda:0\")\n",
    "#         print(f'prefix device is {prefix.device}, t device is {t.device}')\n",
    "        prefix = torch.cat((prefix, t), dim=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"generated tokens numbers {prefix.shape[-1] - seq_len}, accepted_count {accepted_count}, target_sample_count {target_sample_count}, resample_count {resample_count}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Token Generation Speed (with speculative decoding): {max_len/(end_time-start_time)} tokens/s\")\n",
    "    print(f\"Acceptance Rate: {accepted_count/max_len}\")\n",
    "    return prefix\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Replace with the actual IP address of the server\n",
    "def update_target_cache(SERVER_IP,index):\n",
    "    data = {'index': index}\n",
    "    response = requests.post(f'http://{SERVER_IP}:6100/update_cache', json=data)\n",
    "\n",
    "def send_tensor_to_server(SERVER_IP, tensor_list):\n",
    "    data = {'tensor_list': tensor_list}\n",
    "    response = requests.post(f'http://{SERVER_IP}:6100/send_tensor_to_server', json=data)\n",
    "    print('send from edge to server',response.json())\n",
    "\n",
    "def get_tensor(SERVER_IP):\n",
    "    response = requests.get(f'http://{SERVER_IP}:6100/get_tensor_from_server')\n",
    "    tensor_data = response.json()['tensor_list']\n",
    "    if tensor_data is not None:\n",
    "        target_model_cache_history = torch.tensor(tensor_data)\n",
    "        print(f'cache_history from target is {target_model_cache_history.shape}')\n",
    "        return target_model_cache_history\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SERVER_IP = '192.168.0.132'\n",
    "    approx_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "    approx_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", trust_remote_code=True)\n",
    "    input_ids = approx_tokenizer.encode(\"Please write an introduction about UC Irvine: \", return_tensors='pt')\n",
    "    # input_ids = input_ids.to('cuda:1')\n",
    "    # inputs = target_tokenizer(\"Please write an introduction about UC Irvine: \", return_tensors=\"pt\", return_attention_mask=False)\n",
    "    top_k = 20\n",
    "    top_p = 0.9\n",
    "    edge_speculative_sampling(\n",
    "        prefix=input_ids,\n",
    "        approx_model=approx_model,\n",
    "        SERVER_IP= SERVER_IP,\n",
    "        max_len=10,\n",
    "        gamma=4,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
