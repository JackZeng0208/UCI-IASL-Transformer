{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist \n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import threading\n",
    "from utils import KVCacheModel\n",
    "\n",
    "app = Flask(__name__)\n",
    "tensor_lock = threading.Lock()\n",
    "shared_tensor = None\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\",torch_dtype=\"auto\", trust_remote_code=True)\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\", trust_remote_code=True)\n",
    "target_model_cache = KVCacheModel(target_model, 1, 0, 0).to('cuda:0')\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def server_speculative_sampling(draft_tokens : torch.Tensor, \n",
    "                        target_model_cache : torch.nn.Module, \n",
    "                        temperature : float = 1, top_k : int = 0, \n",
    "                        top_p : float = 0, verbose : bool = False, \n",
    "                        random_seed : int = None) -> list  :\n",
    "    \n",
    "    \n",
    "    draft_tokens = draft_tokens.to(\"cuda:0\")\n",
    "    _ = target_model_cache.generate(draft_tokens,1)\n",
    "    target_model_history = target_model_cache._prob_history\n",
    "    target_model_history = target_model_history.to('cpu')\n",
    "    # shape_list = list(target_model_history.size())\n",
    "    return target_model_history.tolist()\n",
    "\n",
    "@app.route('/update_cache', methods=['POST'])\n",
    "def update_cache():\n",
    "    decision = None\n",
    "    rollback_num = request.get_json()\n",
    "    if rollback_num is not None:\n",
    "        target_model_cache.rollback(int(rollback_num['index']))\n",
    "\n",
    "    return jsonify({'message': f'target cache updated successfully {rollback_num}'})\n",
    "\n",
    "@app.route('/send_tensor_to_server', methods=['POST'])\n",
    "def send_tensor():\n",
    "    global shared_tensor\n",
    "    data = request.get_json()\n",
    "    received_tensor = torch.tensor(data['tensor_list'])\n",
    "    \n",
    "    with tensor_lock:\n",
    "        shared_tensor = received_tensor\n",
    "    \n",
    "    return jsonify({'message': f'Tensor received by server successfully {received_tensor}'})\n",
    "\n",
    "@app.route('/get_tensor_from_server', methods=['GET'])\n",
    "def get_tensor():\n",
    "    global shared_tensor\n",
    "    with tensor_lock:\n",
    "        if shared_tensor is not None:\n",
    "            # clone_tensor = shared_tensor.clone().detach() ?\n",
    "            draft_tokens_tensor = torch.tensor(shared_tensor)\n",
    "            history = server_speculative_sampling(\n",
    "                draft_tokens=draft_tokens_tensor,\n",
    "                target_model_cache= target_model_cache,\n",
    "            )\n",
    "            tensor_to_send = history\n",
    "            shared_tensor = None\n",
    "            return jsonify({'tensor_list': tensor_to_send})\n",
    "        else:\n",
    "            return jsonify({'tensor_list': None})\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ipp = '192.168.0.146'\n",
    "    # '192.168.0.239'\n",
    "    ips = \"192.168.0.239\"\n",
    "    # init_processes(0,2,'0.0.0.0',\"1234\")\n",
    "    # print(\"connected\")\n",
    "    app.run(host=ips,port='6100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
